from typing import Literal, TYPE_CHECKING
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from IPython.display import Markdown, display
from langgraph.graph import END

if TYPE_CHECKING:
    from core.schemas import IdiomaticState # For type hinting

# Placeholder for llm_with_tools and save_user_data
# These will be imported or passed as arguments.

IDIOMATIC_BOT_SYSINT = (
  "You are Idiomatic, a friendly and intelligent English language tutor designed to help non-native speakers master idioms through interactive learning. "
  "Your style is warm, supportive, and concise. You adapt to natural language and understand user intent even when it's not phrased exactly. "
  "Your goals: "
  " - Teach idioms through multiple choice questions generated by the system. "
  " - **You MUST use the provided tools when the user's intent matches.** Specifically: "
  "   * User asks for score ('How am I doing?', 'What's my score?'): **MUST call `show_score` tool.** The current score is available in the state. "
  "   * User asks for explanation of the last question ('explain', 'why?', 'explain that'): **MUST call `explain_last_question` tool.** The idiom is available in the state's last_question. "
  "   * User asks to lookup or define an idiom ('What does X mean?', 'lookup Y', 'idiom for Z?'): **MUST call `lookup_idiom` tool** with the user's query. "
  "   * User wants to quit ('Stop', 'I'm done', 'quit', 'exit'): **MUST call `quit_session` tool.** "
  " - Do NOT invent functionality or apologize for missing tools. The tools ARE available. "
  " - Do NOT call tools unnecessarily. If the user is just chatting or providing a quiz answer (a, b, c, d), respond naturally or let the workflow handle the answer. "
  " - Be flexible, encouraging, and gently correct users when needed. "
  "\n\n"
  # "Current Score: {score}. Last Question Idiom: {last_idiom}." # This will be part of the dynamic prompt context
)

def chatbot_node(state: 'IdiomaticState', llm_with_tools_instance, save_user_data_func) -> 'IdiomaticState':
    """Handles initial setup, invokes LLM for routing/chat/tools, and checks for quit signal."""

    # 1. Initial Setup (if name is not set)
    if not state.get("name"):
        print("--- Initial Setup ---")
        user_name = input("ðŸ‘‹ Welcome to Idiomatic! What's your name? ")
        level_choice = input("Skill level (a) beginner / (b) intermediate / (c) advanced: ").strip().lower()
        level_map = {'a': 'beginner', 'b': 'intermediate', 'c': 'advanced'}
        level = level_map.get(level_choice, 'intermediate')
        category = input(f"Preferred idiom category (e.g., business, animals, food) [default: general]: ") or "general"

        state.update({
            "name": user_name,
            "user_level": level,
            "category": category,
            "messages": [AIMessage(content=f"Hello {user_name}! ðŸ‘‹ Let's start with some {category} idioms at the {level} level. I'll ask you multiple-choice questions. You can also ask me to 'explain', show your 'score', 'lookup' an idiom, or 'quit'.")]
        })
        display(Markdown(state['messages'][-1].content))
        return state

    last_message = state["messages"][-1] if state["messages"] else None

    if isinstance(last_message, ToolMessage) and last_message.content == "QUIT_SESSION_SIGNAL":
        print("--- Quit Signal Received ---")
        final_message = "ðŸ‘‹ Thanks for learning with Idiomatic! Your progress is saved."
        state["messages"].append(AIMessage(content=final_message))
        display(Markdown(final_message))
        state["finished"] = True
        save_user_data_func(state) # Use passed function
        return state

    try:
        print("--- Invoking LLM with Tools ---")
        # Construct the dynamic part of the system prompt
        # current_score = state.get('score', 0)
        # last_idiom_name = state.get('last_question', {}).get('idiom', 'None')
        # dynamic_sysint = IDIOMATIC_BOT_SYSINT + f"\nCurrent Score: {current_score}. Last Question Idiom: {last_idiom_name}."

        # The llm_with_tools is expected to be pre-configured with tools that can access state.
        # If tools need direct access to llm, it's handled by the tool itself.
        # For the chatbot node, the system prompt needs access to score and last idiom.
        current_score = state.get('score', 0)
        last_idiom_name = state.get('last_question', {}).get('idiom', 'None')

        # The IDIOMATIC_BOT_SYSINT itself might contain placeholders like {score} and {last_idiom}
        # or we append them. The original had them appended.
        # Let's ensure IDIOMATIC_BOT_SYSINT does not have these placeholders to avoid confusion.
        # And append them dynamically here.

        dynamic_prompt_context = f"\nCurrent Score: {current_score}. Last Question Idiom: {last_idiom_name}."
        final_system_prompt = IDIOMATIC_BOT_SYSINT + dynamic_prompt_context

        # Ensure tools are correctly bound if they need llm_instance.
        # This will be handled by how llm_with_tools_instance is created.
        # The tool functions themselves (explain_last_question, lookup_idiom) were defined
        # in tools/idiom_tools.py to accept llm_instance.
        # The main script will use functools.partial to bind the llm_instance to these tools
        # before they are passed to llm.bind_tools() and then to ToolNode.

        response = llm_with_tools_instance.invoke(
            [SystemMessage(content=final_system_prompt)] + state["messages"]
        )
        state["messages"].append(response)

        if not response.tool_calls:
            display(Markdown(f"**Idiomatic:** {response.content}"))

    except Exception as e:
        print(f"Error invoking LLM in chatbot_node: {e}")
        error_msg = "Sorry, I encountered an error. Please try again."
        state["messages"].append(AIMessage(content=error_msg))
        display(Markdown(error_msg))
    return state

def get_user_input(state: 'IdiomaticState') -> 'IdiomaticState':
    print("--- Waiting for User Input ---")
    prompt_message = "Your answer (a/b/c/d) or command: \n"
    user_input = input(prompt_message).strip()
    state["messages"].append(HumanMessage(content=user_input))
    return state

def route_logic(state: 'IdiomaticState') -> Literal["tools", "evaluate_quiz", "chatbot_node", "generate_question", END]:
    print(f"--- Routing (Finished: {state.get('finished')}) ---")
    if state.get("finished"):
        print("--- Routing to END ---")
        return END

    last_message = state["messages"][-1] if state["messages"] else None

    if isinstance(last_message, AIMessage) and "Let's start" in last_message.content:
         print("--- Routing: Initial Setup -> Generate Question ---")
         return "generate_question"

    if isinstance(last_message, AIMessage) and last_message.tool_calls:
        print("--- Routing: AIMessage with Tool Calls -> Tools Node ---")
        # The tool calls will include the tool name and arguments.
        # For tools that need `llm_instance` (explain_last_question, lookup_idiom),
        # the `ToolNode` execution must somehow provide this.
        # This is the tricky part with the current setup.
        # Standard `ToolNode` does not pass extra arguments like an `llm_instance` to tools.
        # The tools should ideally be self-contained or use a globally accessible LLM.
        return "tools"

    if isinstance(last_message, ToolMessage):
         print("--- Routing: ToolMessage -> Chatbot Node ---")
         return "chatbot_node"

    if isinstance(last_message, HumanMessage):
        content = last_message.content.strip().lower()
        if content in {"a", "b", "c", "d"} and state.get("last_question"):
             print("--- Routing: Human Answer -> Evaluate Quiz ---")
             return "evaluate_quiz"
        else:
             print("--- Routing: Human Command/Chat -> Chatbot Node ---")
             return "chatbot_node"

    if isinstance(last_message, AIMessage) and not last_message.tool_calls:
         print("--- Routing: AIMessage (Eval Result/Chat/Explanation) -> Generate Question ---")
         return "generate_question"

    print("--- Routing: Fallback -> Generate Question ---")
    return "generate_question"

# Note: The issue with `llm_instance` for tools needs to be resolved.
# Option 1: Tools in `idiom_tools.py` import and use a global `llm` (initialized in main script).
# Option 2: Modify `ToolNode` or create a custom one that can pass context. (More complex)
# Option 3: Curry the tools with the `llm_instance` when they are defined or when `llm_with_tools` is created.
# Example for Option 3 (in main script or graph.py):
# from functools import partial
# bound_explain_tool = partial(explain_last_question, llm_instance=llm)
# bound_lookup_tool = partial(lookup_idiom, llm_instance=llm)
# tools = [show_score, bound_explain_tool, bound_lookup_tool, quit_session]
# llm_with_tools = llm.bind_tools(tools)
# tool_node = ToolNode(tools)
# This seems the most viable approach. The `agent.py` itself doesn't need to change much then,
# but the tool definitions and how they are passed to `llm.bind_tools` will.
# For now, I'll proceed with creating agent.py as is, and we'll address the tool binding during graph setup or main script refactoring.
